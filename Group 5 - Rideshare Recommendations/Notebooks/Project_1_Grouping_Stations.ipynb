{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_rows',400)\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in one week of turnstile data to extract station names\n",
    "Source: Richard Chiou\n",
    "\"\"\"\n",
    "\n",
    "def get_data(week_nums):\n",
    "    url = \"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt\"\n",
    "    dfs = []\n",
    "    for week_num in week_nums:\n",
    "        file_url = url.format(week_num)\n",
    "        dfs.append(pd.read_csv(file_url))\n",
    "    return pd.concat(dfs)\n",
    "        \n",
    "week_nums = [160903]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "source: https://stackoverflow.com/questions/43892459/check-if-geo-point-is-inside-or-outside-of-polygon\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "def isPointinPolygon(lat, long, coord_arr):\n",
    "    polygon = Polygon(coord_arr)\n",
    "    point = Point(long,lat)\n",
    "    return point.within(polygon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the goals of our project was to determine which neighbhorhoods had the greatest decrease in MTA ridership. In order to do that, we needed to assign each station to a neighborhood.\n",
    "\n",
    "In this notebook, the MTA turnstile data set was normalized to the subway station coordinates data set by station name. This allows for assignment of coordinates to each station in the turnstile data set, followed by assignment of neighborhood to each station.\n",
    "\n",
    "Station names were normalized by:\n",
    "   1. First transformation: character regex\n",
    "   2. Final transformation: re-ordering each string to [0-9][a-z] \n",
    "   3. Using fuzzywuzzy for fuzzy string match of final transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Regex dict for data cleaning\"\"\"\n",
    "\n",
    "replace_values = {' - ': '_',\n",
    "                  '-': '_',\n",
    "                  ' ': '_',\n",
    "                  '/': '_',\n",
    "                  '--': '_',\n",
    "                  r\"\\(\": \"\",\n",
    "                  r\"\\)\": \"\"       \n",
    "                 }             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Character regex for both datasets\n",
    "\"\"\"\n",
    "\n",
    "station_names = pd.read_csv(\"http://web.mta.info/developers/data/nyct/subway/Stations.csv\")\n",
    "station_names_stop_names = pd.DataFrame(station_names['Stop Name'].unique(), columns=['original_station_names'])\n",
    "station_names_stop_names['first_clean'] = station_names_stop_names.original_station_names.str.upper().replace(replace_values, regex=True)\n",
    "\n",
    "turnstiles_df = get_data(week_nums)\n",
    "turnstiles_df['StationLine'] = turnstiles_df.STATION + turnstiles_df.LINENAME\n",
    "turnstile_station_names = pd.DataFrame(turnstiles_df.STATION.unique(),columns=['original_turnstile'])\n",
    "turnstile_station_names['first_clean'] = turnstile_station_names.original_turnstile.str.upper().replace(replace_values, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-ordering of station coordinates dataset\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Splitting cleaned name into delimit by _\"\"\"\n",
    "split_name = station_names_stop_names.first_clean.str.split(pat='_',expand=True)\n",
    "\n",
    "\"\"\"Handling numeric portion of split,cleaned name\"\"\"\n",
    "num_split = pd.DataFrame()\n",
    "for i in range(0,len(split_name.columns-1)):\n",
    "    num_split[i] = split_name[i].str.findall(r'(\\d+)')\n",
    "num_split_list = num_split.values.tolist()\n",
    "\n",
    "for i in range(len(num_split_list)):\n",
    "    num_split_list[i] = [x for x in num_split_list[i] if x]\n",
    "\n",
    "num_split_clean = pd.DataFrame(num_split_list)\n",
    "for i in range(len(num_split_clean.columns)):\n",
    "    num_split_clean[i] = num_split_clean[i].str[0]\n",
    "num_split_clean['cleaned_num'] = num_split_clean[num_split_clean.columns[:]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "\"\"\"Handling char portion of split, cleaned name\"\"\"\n",
    "char_split = pd.DataFrame()\n",
    "for i in range(0,len(split_name.columns-1)):\n",
    "    char_split[i] = split_name[i].str.findall(r'(\\D+)')\n",
    "char_split_list = char_split.values.tolist()\n",
    "\n",
    "for i in range(len(char_split_list)):\n",
    "    char_split_list[i] = [x for x in char_split_list[i] if x!=[]]\n",
    "\n",
    "char_split_clean = pd.DataFrame(char_split_list)\n",
    "\n",
    "for i in range(len(num_split_clean.columns)):\n",
    "    char_split_clean[i] = char_split_clean[i].str[0]\n",
    "\n",
    "char_split_clean['cleaned_num']=char_split_clean[char_split_clean.columns[:]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "station_names_stop_names['final_clean_station_names'] = (num_split_clean.cleaned_num + char_split_clean.cleaned_num).str.replace('[','',regex=True).str.replace(']','',regex=True)\n",
    "cleaned_station_name_data = station_names_stop_names.sort_values(by=['final_clean_station_names']).reset_index()\n",
    "\n",
    "#station_names_stop_names[station_names_stop_names.duplicated(['final_clean_station_names'])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>original_station_names</th>\n",
       "      <th>first_clean</th>\n",
       "      <th>final_clean_station_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, original_station_names, first_clean, final_clean_station_names]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Duplication check of transformed string in station names\n",
    "\"\"\"\n",
    "cleaned_station_name_data[cleaned_station_name_data.duplicated(['final_clean_station_names'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-ordering of MTA turnstiles dataset\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Splitting cleaned name into delimit by _\"\"\"\n",
    "split_name = turnstile_station_names.first_clean.str.split(pat='_',expand=True)\n",
    "\n",
    "\"\"\"Handling numeric portion of split,cleaned name\"\"\"\n",
    "num_split = pd.DataFrame()\n",
    "for i in range(0,len(split_name.columns-1)):\n",
    "    num_split[i] = split_name[i].str.findall(r'(\\d+)')\n",
    "num_split_list = num_split.values.tolist()\n",
    "\n",
    "for i in range(len(num_split_list)):\n",
    "    num_split_list[i] = [x for x in num_split_list[i] if x]\n",
    "\n",
    "num_split_clean = pd.DataFrame(num_split_list)\n",
    "for i in range(len(num_split_clean.columns)):\n",
    "    num_split_clean[i] = num_split_clean[i].str[0]\n",
    "num_split_clean['cleaned_num'] = num_split_clean[num_split_clean.columns[:]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "\"\"\"Handling char portion of split, cleaned name\"\"\"\n",
    "char_split = pd.DataFrame()\n",
    "for i in range(0,len(split_name.columns-1)):\n",
    "    char_split[i] = split_name[i].str.findall(r'(\\D+)')\n",
    "char_split_list = char_split.values.tolist()\n",
    "\n",
    "for i in range(len(char_split_list)):\n",
    "    char_split_list[i] = [x for x in char_split_list[i] if x!=[]]\n",
    "\n",
    "char_split_clean = pd.DataFrame(char_split_list)\n",
    "\n",
    "for i in range(len(num_split_clean.columns)):\n",
    "    char_split_clean[i] = char_split_clean[i].str[0]\n",
    "\n",
    "char_split_clean['cleaned_num']=char_split_clean[char_split_clean.columns[:]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "\"\"\"Concat cleaned num and cleaned char\"\"\"\n",
    "\n",
    "turnstile_station_names['final_clean_station_names_turnstiles'] = (num_split_clean.cleaned_num + char_split_clean.cleaned_num).str.replace('[','',regex=True).str.replace(']','',regex=True)\n",
    "cleaned_turnstile_data = turnstile_station_names.sort_values(by=['final_clean_station_names_turnstiles']).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>original_turnstile</th>\n",
       "      <th>first_clean</th>\n",
       "      <th>final_clean_station_names_turnstiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>221</td>\n",
       "      <td>4 AV-9 ST</td>\n",
       "      <td>4_AV_9_ST</td>\n",
       "      <td>4_9AV_ST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index original_turnstile first_clean final_clean_station_names_turnstiles\n",
       "94    221          4 AV-9 ST   4_AV_9_ST                             4_9AV_ST"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Duplication check of transformed string in MTA turnstile dataset\n",
    "\n",
    "Can ignore this case, manual check conducted for duplication case\n",
    "\n",
    "\"\"\"\n",
    "cleaned_turnstile_data[cleaned_turnstile_data.duplicated(['final_clean_station_names_turnstiles'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate final table of original names with transformed names for both data sets\n",
    "\"\"\"\n",
    "\n",
    "cleaned_concat = pd.concat([cleaned_station_name_data,cleaned_turnstile_data], axis=1)[['original_station_names','final_clean_station_names', 'original_turnstile','final_clean_station_names_turnstiles']].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use fuzzywuzzy for string match of final transformed names for both data sets\n",
    "\"\"\"\n",
    "\n",
    "final_match = []\n",
    "\n",
    "for turnstile_name_clean in list(cleaned_concat.final_clean_station_names_turnstiles):\n",
    "    station_name_clean = process.extractOne(turnstile_name_clean, list(cleaned_concat.final_clean_station_names),scorer=fuzz.token_sort_ratio)[0]\n",
    "    turnstile_name_match = cleaned_concat.original_turnstile[cleaned_concat.final_clean_station_names_turnstiles == turnstile_name_clean].values[0]\n",
    "    station_name_match = cleaned_concat.original_station_names[cleaned_concat.final_clean_station_names == station_name_clean].values[0]\n",
    "    final_match.append([turnstile_name_match,station_name_match])\n",
    "\n",
    "final_match_dict = pd.DataFrame(final_match,columns=['Turnstile_Station_Name','Station_Name']).replace(r'^\\s*$', np.nan, regex=True).dropna().sort_values(by=['Station_Name']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in and normalize station coordinate data\n",
    "\"\"\"\n",
    "\n",
    "station_names.columns = ['Station_ID','Complex_ID','Stop_ID','Division','Line','StopName','Borough','DayTimeRoutes','Structure','Latitude','Longitude','NorthDirectionLabel','SouthDirectionLabel']\n",
    "\n",
    "station_names['StopName'] = station_names.StopName.astype(str)\n",
    "station_names['Latitude'] = station_names.Latitude.astype(str)\n",
    "station_names['Longitude'] = station_names.Longitude.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names['merged'] = station_names[['StopName','Latitude','Longitude']].apply(lambda x: '_'.join(x), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StopName</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Astoria - Ditmars Blvd</td>\n",
       "      <td>40.775036</td>\n",
       "      <td>-73.91203399999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Astoria Blvd</td>\n",
       "      <td>40.770258</td>\n",
       "      <td>-73.917843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30 Av</td>\n",
       "      <td>40.766779</td>\n",
       "      <td>-73.921479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Broadway</td>\n",
       "      <td>40.76182</td>\n",
       "      <td>-73.925508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36 Av</td>\n",
       "      <td>40.756803999999995</td>\n",
       "      <td>-73.929575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Prince's Bay</td>\n",
       "      <td>40.525507</td>\n",
       "      <td>-74.200064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Pleasant Plains</td>\n",
       "      <td>40.52241</td>\n",
       "      <td>-74.21784699999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Richmond Valley</td>\n",
       "      <td>40.519631</td>\n",
       "      <td>-74.229141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Tottenville</td>\n",
       "      <td>40.512764000000004</td>\n",
       "      <td>-74.251961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Arthur Kill</td>\n",
       "      <td>40.516578</td>\n",
       "      <td>-74.24209599999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>493 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   StopName           Longitude            Latitude\n",
       "0    Astoria - Ditmars Blvd           40.775036  -73.91203399999999\n",
       "1              Astoria Blvd           40.770258          -73.917843\n",
       "2                     30 Av           40.766779          -73.921479\n",
       "3                  Broadway            40.76182          -73.925508\n",
       "4                     36 Av  40.756803999999995          -73.929575\n",
       "..                      ...                 ...                 ...\n",
       "488            Prince's Bay           40.525507          -74.200064\n",
       "489         Pleasant Plains            40.52241  -74.21784699999999\n",
       "490         Richmond Valley           40.519631          -74.229141\n",
       "491             Tottenville  40.512764000000004          -74.251961\n",
       "492             Arthur Kill           40.516578  -74.24209599999999\n",
       "\n",
       "[493 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_locations = pd.DataFrame(station_names.merged.unique())[0].str.split(pat='_',expand=True)\n",
    "station_locations.columns = ['StopName','Longitude','Latitude']\n",
    "\n",
    "station_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Matching coordinates to MTA turnstile station names\n",
    "\"\"\"\n",
    "\n",
    "merged_station_location = []\n",
    "for station_name in list(final_match_dict.Station_Name):\n",
    "    turnstile_name_final = final_match_dict.Turnstile_Station_Name[final_match_dict.Station_Name == station_name].values[0]\n",
    "    longitude = station_locations.Longitude[station_locations.StopName == station_name].values[0]\n",
    "    latitude = station_locations.Latitude[station_locations.StopName == station_name].values[0]\n",
    "    merged_station_location.append([turnstile_name_final,longitude,latitude])\n",
    "    \n",
    "final_merged_station_location = pd.DataFrame(merged_station_location,columns=['TurnstileData_StationName','Longitude','Latitude'])\n",
    "final_merged_station_location.to_csv(\"station_locations_dict.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regex dict for normalizing station coordinate data set\n",
    "\"\"\"\n",
    "\n",
    "replace_values_neighborhoods = {r\"\\(\": \"\",\n",
    "                                r\"\\)\": \"\",\n",
    "                                'MULTIPOLYGON ': '',\n",
    "                               ', ': ',',\n",
    "                               ' ':','}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regex normalization of neighborhoods data set\n",
    "\"\"\"\n",
    "\n",
    "neighborhoods_df = pd.read_csv('nynta.csv')[['BoroName','NTAName','the_geom']]\n",
    "neighborhoods_df.columns = ['Borough','NTA','geom']\n",
    "neighborhoods_df = neighborhoods_df.replace(replace_values_neighborhoods,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reformat coordinate array from neighborhoods data set for polygon mapping\n",
    "\"\"\"\n",
    "\n",
    "coord_array_list = []\n",
    "for coords in list(neighborhoods_df.geom):\n",
    "    split = np.char.split(coords, sep=',').tolist()\n",
    "    split_list = [[i] for i in split]\n",
    "    coord_array = np.array([j+i for i,j in zip(split_list[::2], split_list[1::2])])\n",
    "    coord_array_list.append(coord_array)\n",
    "    \n",
    "neighborhoods_df['geom_array'] = pd.Series(coord_array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_df.to_csv(\"NTA_locations_polygons.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For every station, assign station to nieghborhood\n",
    "\"\"\"\n",
    "\n",
    "final_assign = []\n",
    "\n",
    "for station_row in final_merged_station_location.itertuples():\n",
    "    for array in neighborhoods_df.geom_array:\n",
    "        if isPointinPolygon(float(station_row.Latitude), float(station_row.Longitude), array):\n",
    "            nta = neighborhoods_df.NTA[neighborhoods_df.geom_array.astype(str) == str(array)].values[0]\n",
    "            borough = neighborhoods_df.Borough[neighborhoods_df.geom_array.astype(str)==str(array)].values[0]\n",
    "            final_assign.append([station_row.TurnstileData_StationName,nta,borough])\n",
    "\n",
    "\n",
    "final = pd.DataFrame(final_assign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Manual match of remaining strings\n",
    "\"\"\"\n",
    "final.columns=['Station','NTA','Borough']\n",
    "success_stations = pd.Series(final.Station.unique())\n",
    "need_to_match_stations = final_merged_station_location.TurnstileData_StationName[final_merged_station_location.TurnstileData_StationName.isin(success_stations)==False].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For strings with more than 1 fuzzywuzzy best match, rank matches by occurance and pick match with best rank\n",
    "\"\"\"\n",
    "\n",
    "NTA_counts = pd.DataFrame(final.NTA.value_counts()).reset_index()\n",
    "NTA_counts.columns = ['NTA','counts']\n",
    "counts_list = []\n",
    "for rows in final.itertuples():\n",
    "    count = NTA_counts.counts[NTA_counts.NTA == rows.NTA]\n",
    "    counts_list.append(count.values[0])\n",
    "    \n",
    "counts_series = pd.Series(counts_list)\n",
    "\n",
    "final['counts_rank'] = counts_series\n",
    "station_neighborhood_assignments = final.groupby(['Station'])[['NTA','counts_rank']].max().reset_index().drop(['counts_rank'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Completion of matchlist\n",
    "\"\"\"\n",
    "\n",
    "manual_match = pd.DataFrame({'Station':['4AV-9 ST','EASTN PKWY-MUSM','RIT-ROOSEVELT'], 'NTA':['Park Slope-Gowanus','Prospect Heights','Lenox Hill-Roosevelt Island']})\n",
    "station_neighborhood_assignments = station_neighborhood_assignments.append(manual_match).reset_index()\n",
    "station_neighborhood_assignments.NTA = station_neighborhood_assignments.NTA.str.upper().replace(',',' ',regex=True)\n",
    "station_neighborhood_assignments = station_neighborhood_assignments[['Station','NTA']]\n",
    "station_neighborhood_assignments.to_csv('station_neighborhood_assignments.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
